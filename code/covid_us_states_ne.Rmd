---
title: "VAR for Covid Cases"
author: "Sarah McDonald"
date: "4/27/2022"
output: 
  #html_document
  md_document:
    variant: markdown_github
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE, 
                      message = FALSE)
```




```{r}
library(tidyverse)
library(MASS, exclude = "select")
library(here)
library(zoo)
library(vars)
library(tseries)
library(ggplot2)
library(lubridate)
```






The data is from the [CDC](https://data.cdc.gov/Case-Surveillance/United-States-COVID-19-Cases-and-Deaths-by-State-o/9mfq-cb36/data). Note: New York City is tabulated separately from New York State.

```{r}
df <- read.csv(here("data/United_States_COVID-19_Cases_and_Deaths_by_State_over_Time.csv")) %>%
  select(submission_date, state, new_case)
head(df) 
```

For simplicity, start with a subset of northeastern states, with counts by month starting in March 2020. We will look at Covid cases instead of deaths.


```{r}
# make table of states/regions
states <- data.frame(state.name, state.abb)
northeast_states_df <- states %>%
  filter(state.region == "Northeast")
northeast_states <- northeast_states_df %>%
  select(state.abb) %>%
  rename("state" = "state.abb") %>%
  pull(state)

northeast_states <- c(northeast_states, "NYC")

# make northeast df
northeast_df <- df %>%
  # select northeast states only
  filter(state %in% northeast_states) %>%
  select(submission_date, new_case, state) %>%
  rename( "cases" = "new_case") %>%
  mutate(date = mdy(submission_date)) %>%
  select(state, cases, date) %>%
  group_by(date, state) %>%
  # calculate cases by month
  summarize(cases = sum(cases), .groups = "drop") %>%
  pivot_wider(names_from = state, values_from = cases) 
```

Next we convert to a time series object.

```{r}
# make each individual state a time series
ts_list <- lapply(X = setNames(northeast_states, northeast_states), FUN =  function(x) {
  state_ts <- ts(northeast_df[[x]], 
        frequency = 1)
  state_ts
})

# merge time series together into one time series object/matrix
var_data <- do.call(ts.union, ts_list)
```

Next we calculate a correlation between states. 
```{r}
cor_matrix <- round(cor(var_data), 3)
cor_matrix
ggcorrplot::ggcorrplot(cor_matrix, outline.col = "white", lab = T, type = "lower")
```

I use `VARselect` to see the information criteria for different lags. I choose 10.

```{r}
VARselect(var_data)
```



VAR does the estimation of VAR with OLS. 

```{r}
var_est <- VAR(y = var_data, p = 10)


# coefficients for each equation, CA example
summary <- summary(var_est)

summary$varresult$PA
```




Next we run Phillips-Perron Unit Root Test, which tests the stationarity assumption.
The results of the test suggest that the data is stationary, if we hold significance at the 0.1 level.

```{r}
pp_test <- lapply(ts_list, pp.test)

lapply(pp_test, "[[", "p.value") %>%
  as.data.frame() %>%
  pivot_longer(everything(), names_to = c("state"), values_to = "p.value") %>%
  mutate(p.value = round(p.value, 3))
```

The stability function checks for structural breaks. Structural breaks may impact the estimation. The line in the middle of the plot should not go outside of the red bounds.

```{r}
stability <- stability(var_est, type = "OLS-CUSUM")

# connecticut for example
stability$stability$PA
plot(stability$stability$PA)
```



`serial.test()` computes the multivariate Portmanteau- and Breusch-Godfrey test for serially correlated errors. This checks the assumption that the residuals should be non-autocorrelated. The null hypothesis is that there is no serial correlation (need to check this). So we conclude, that there is serial correlation in this data.

```{r}
serial.test(var_est, type = "PT.asymptotic")
```


`arch.test()` this computes the ARCH(autoregressive conditionally heteroscedastic)-LM test, which analyzes volatility variance. Need to check what the null is for this test.

```{r}
arch.test(var_est, 
          multivariate.only = TRUE)
```


`causality()` computes Granger- and Instantaneous causality. Granger causality tests if one time series is useful for forecasting another. We would say that Pensylvaina does Granger-cause COVID cases in other states.

```{r}
causality(var_est, cause = "PA")
```



`normality.test()` checks for normality of the distribution of the residuals. It's not clear what the null/alternative hypothesis is. I think it is that the null hypothesis is that the distribution is normal, so we would conclude that the residuals are not normal.
```{r}
normality.test(var_est, multivariate.only = TRUE)
```



`irf()` computes the impulse response coefficients. It is not clear to my why we would need this.

```{r, eval = F}
irf(var_est, impulse = "PA", response = "MA", n.ahead = 20, boot = TRUE)

```


`fevd()` computes the forecast error variance decomposition. It tells which states influence the variance the most over time.

```{r}
fevd_out <- fevd(var_est)
fevd_out$PA
```


### New Task

Use Sept 2020-Sept 2021 data

Predict October 1, then October 2, and so on.


```{r}
# one year of data

data = northeast_df
prediction_date = "2021-10-07"
lag = 10
period = 7

prediction_performance <- function(data, prediction_date, lag, period){

  prediction_date = as.Date(prediction_date)
  actual_start = "2020-09-30"
  actual_end = prediction_date - period
  
  actual_data_train <- data %>%
    filter(date >= actual_start & date <= actual_end)
  
  # make each individual state a time series
  ts_list <- lapply(X = setNames( colnames(data) [!(colnames(data) %in% "date")],  colnames(data) [!(colnames(data) %in% "date")]), FUN =  function(x) {
    state_ts <- ts(actual_data_train[[x]], 
          frequency = 1)
    state_ts
  })
  
  var_data <- do.call(ts.union, ts_list)
  
  model <- VAR(var_data, p = lag)
  
  prediction <- predict(model, n.ahead = period)
  
  predicted_t_7 <-lapply(prediction$fcst, "[[", 7)  %>%
    as.data.frame() %>%
    pivot_longer(everything(), names_to = c("state"), values_to = "predicted_t_7")
  
  actual_t_7 <- data %>%
    filter(date == prediction_date) %>%
    select(!date) %>%
    pivot_longer(everything(), names_to = "state", values_to = "actual_t_7")
  
  
  comparison <- merge(predicted_t_7, actual_t_7, by = "state", all = T) %>%
    mutate(diff = actual_t_7- predicted_t_7, 
           diff_squared = diff^2)
  
  rmse = sqrt(mean(comparison$diff_squared))
  
  
  final <- list(prediction_date = prediction_date, 
                actual_start = actual_start, 
                actual_end = actual_end, 
                lag = lag, 
                period = period, 
                rmse = rmse)
  
  return(final)
}






out <- lapply(X = seq(as.Date("2021-10-01"), as.Date("2021-10-10"), 1), 
               FUN = function(x) prediction_performance(data = northeast_df, 
                                                        prediction_date = x, 
                                                        lag = 10, 
                                                        period = 7))
do.call(rbind, out)
```







