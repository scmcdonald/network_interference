---
title: "VAR for Covid Cases"
author: "Sarah McDonald"
date: "4/27/2022"
output: 
  html_document
  #md_document:
   # variant: markdown_github
#always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE, 
                      message = FALSE)
```


```{r}
library(tidyverse)
library(MASS, exclude = "select")
library(here)
library(zoo)
library(vars)
library(tseries)
library(ggplot2)
library(lubridate)
```






The data is from the [CDC](https://data.cdc.gov/Case-Surveillance/United-States-COVID-19-Cases-and-Deaths-by-State-o/9mfq-cb36/data).

```{r}
df <- read.csv(here("data/United_States_COVID-19_Cases_and_Deaths_by_State_over_Time.csv")) %>%
  select(submission_date, state, new_case)
head(df) 
```

For simplicity, start with a subset of northeastern states, with counts by month starting in March 2020. We will look at Covid cases instead of deaths.


```{r}
# make table of states/regions
states <- data.frame(state.name, state.region, state.abb)
northeast_states_df <- states %>%
  filter(state.region == "Northeast")
northeast_states <- northeast_states_df %>%
  select(state.name) %>%
  rename("state" = "state.name") %>%
  pull(state)

# make northeast df
northeast_df <- df %>%
  # select northeast states only
  merge(northeast_states_df, by.x = "state", by.y = "state.abb", all.x = F) %>%
  select(submission_date, new_case, state.name) %>%
  rename("state" = "state.name",  "cases" = "new_case") %>%
  mutate(date = mdy(submission_date)) %>%
  select(state, cases, date) %>%
  group_by(date, state) %>%
  # calculate cases by month
  summarize(cases = sum(cases), .groups = "drop") %>%
  pivot_wider(names_from = state, values_from = cases) 
```

Next we convert to a time series object.

```{r}
# make each individual state a time series
ts_list <- lapply(X = setNames(northeast_states, northeast_states), FUN =  function(x) {
  state_ts <- ts(northeast_df[[x]], 
        frequency = 1)
  state_ts
})

# merge time series together into one time series object/matrix
var_data <- do.call(ts.union, ts_list)
```

Next we calculate a correlation between states. I get errors later about singularity and positive definite. 
So, I calculate the correlation matrix and take the determinant, and find that it is treated as zero by machine precision.

```{r}
cor_matrix <- round(cor(var_data), 3)
cor_matrix
# I think this is the problem -> 0 with machine precision
print(paste("The determinant is:", det(cor_matrix)))

```

I use `VARselect` to see the information criteria for different lags. All agree that the lag should be 2.

```{r}
VARselect(var_data)
```

VAR does the estimation of VAR with OLS. 

```{r}
var_est <- VAR(y = var_data, p = 10)


# coefficients for each equation, CA example
summary <- summary(var_est)
summary$varresult$Pennsylvania
```




Next we run Phillips-Perron Unit Root Test, which tests the stationarity assumption.
The results of the test suggest that the data is stationary, if we hold significance at the 0.1 level.

```{r}
pp_test <- lapply(ts_list, pp.test)

lapply(pp_test, "[[", "p.value") %>%
  as.data.frame() %>%
  pivot_longer(everything(), names_to = c("state"), values_to = "p.value") %>%
  mutate(p.value = round(p.value, 3))
```

The stability function checks for structural breaks. Structural breaks may impact the estimation. The line in the middle of the plot should not go outside of the red bounds.

```{r}
stability <- stability(var_est, type = "OLS-CUSUM")

# connecticut for example
stability$stability$Pennsylvania
plot(stability$stability$Pennsylvania)
```



`serial.test()` computes the multivariate Portmanteau- and Breusch-Godfrey test for serially correlated errors. This checks the assumption that the residuals should be non-autocorrelated. The null hypothesis is that there is no serial correlation (need to check this). So we conclude, that there is serial correlation in this data.

```{r}
serial.test(var_est, type = "PT.asymptotic")
```


`arch.test()` this computes the ARCH(autoregressive conditionally heteroscedastic)-LM test, which analyzes volatility variance. Need to check what the null is for this test.

```{r}
arch.test(var_est, 
          multivariate.only = TRUE)
```


`causality()` computes Granger- and Instantaneous causality. Granger causality tests if one time series is useful for forecasting another. We would say that Pensylvaina does Granger-cause COVID cases in other states.

```{r}
causality(var_est, cause = "Pennsylvania")
```



`normality.test()` checks for normality of the distribution of the residuals. It's not clear what the null/alternative hypothesis is. I think it is that the null hypothesis is that the distribution is normal, so we would conclude that the residuals are not normal.
```{r}
normality.test(var_est, multivariate.only = TRUE)
```



`irf()` computes the impulse response coefficients. It is not clear to my why we would need this.

```{r, eval = F}
irf(var_est, impulse = "Connecticut", response = "Maine", n.ahead = 20, boot = TRUE)

```


`fevd()` computes the forecast error variance decomposition. It tells which states influence the variance the most over time.

```{r}
fevd(var_est)
```




